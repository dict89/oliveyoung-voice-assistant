#!/usr/bin/env python3
"""
ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ì˜ˆì‹œ:
  python crawl.py --store "ëª…ë™ íƒ€ìš´"
  python crawl.py --stores "ëª…ë™ íƒ€ìš´,ê°•ë‚¨ì—­ì ,í™ëŒ€ì…êµ¬ì "
  python crawl.py --store "ëª…ë™ íƒ€ìš´" --categories "ìŠ¤í‚¨ì¼€ì–´,ë©”ì´í¬ì—…" --headless
"""

import asyncio
import argparse
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.oliveyoung_crawler import OliveYoungCrawler


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='ì˜¬ë¦¬ë¸Œì˜ ë§¤ì¥ ìƒí’ˆ ì •ë³´ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s --store "ëª…ë™ íƒ€ìš´"
  %(prog)s --stores "ëª…ë™ íƒ€ìš´,ê°•ë‚¨ì—­ì ,í™ëŒ€ì…êµ¬ì "
  %(prog)s --store "ëª…ë™ íƒ€ìš´" --categories "ìŠ¤í‚¨ì¼€ì–´,ë©”ì´í¬ì—…"
  %(prog)s --store "ëª…ë™ íƒ€ìš´" --headless
        """
    )
    
    # ë§¤ì¥ ì˜µì…˜
    store_group = parser.add_mutually_exclusive_group(required=True)
    store_group.add_argument(
        '--store',
        type=str,
        help='í¬ë¡¤ë§í•  ë‹¨ì¼ ë§¤ì¥ëª… (ì˜ˆ: "ëª…ë™ íƒ€ìš´")'
    )
    store_group.add_argument(
        '--stores',
        type=str,
        help='í¬ë¡¤ë§í•  ì—¬ëŸ¬ ë§¤ì¥ëª… (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: "ëª…ë™ íƒ€ìš´,ê°•ë‚¨ì—­ì ")'
    )
    
    # ì¹´í…Œê³ ë¦¬ ì˜µì…˜
    parser.add_argument(
        '--categories',
        type=str,
        help='í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: "ìŠ¤í‚¨ì¼€ì–´,ë©”ì´í¬ì—…")\n'
             'ê¸°ë³¸ê°’: ì „ì²´ ì¹´í…Œê³ ë¦¬',
        default=None
    )
    
    # ë¸Œë¼ìš°ì € ì˜µì…˜
    parser.add_argument(
        '--headless',
        action='store_true',
        help='ë¸Œë¼ìš°ì €ë¥¼ ìˆ¨ê¹€ ëª¨ë“œë¡œ ì‹¤í–‰ (ê¸°ë³¸: ë¸Œë¼ìš°ì € í‘œì‹œ)'
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument(
        '--output-dir',
        type=str,
        default='data',
        help='í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)'
    )
    
    # ëŒ€ê¸° ì‹œê°„
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='ìš”ì²­ ì‚¬ì´ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 2.0)'
    )
    
    return parser.parse_args()


async def crawl_single_store(crawler: OliveYoungCrawler, 
                             store_name: str, 
                             categories: list = None):
    """ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§"""
    print(f"\n{'='*70}")
    print(f"ğŸª '{store_name}' ë§¤ì¥ í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'='*70}\n")
    
    try:
        products = await crawler.get_store_products(
            store_name=store_name,
            categories=categories
        )
        
        # ê²°ê³¼ ìš”ì•½
        total_products = sum(len(items) for items in products['categories'].values())
        print(f"\nâœ… '{store_name}' í¬ë¡¤ë§ ì™„ë£Œ:")
        print(f"   - ì´ ì¹´í…Œê³ ë¦¬: {len(products['categories'])}ê°œ")
        print(f"   - ì´ ìƒí’ˆ: {total_products}ê°œ")
        
        for category, items in products['categories'].items():
            print(f"   - {category}: {len(items)}ê°œ")
        
        return products
        
    except Exception as e:
        print(f"\nâŒ '{store_name}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None


async def crawl_multiple_stores(crawler: OliveYoungCrawler, 
                                store_names: list,
                                categories: list = None,
                                delay: float = 2.0):
    """ì—¬ëŸ¬ ë§¤ì¥ í¬ë¡¤ë§"""
    results = []
    
    for i, store_name in enumerate(store_names, 1):
        print(f"\n[{i}/{len(store_names)}]")
        
        result = await crawl_single_store(crawler, store_name, categories)
        results.append(result)
        
        # ë§ˆì§€ë§‰ ë§¤ì¥ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
        if i < len(store_names):
            print(f"\nâ³ {delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(delay)
    
    return results


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_args()
    
    # ì¹´í…Œê³ ë¦¬ íŒŒì‹±
    categories = None
    if args.categories:
        categories = [cat.strip() for cat in args.categories.split(',')]
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = OliveYoungCrawler(output_dir=args.output_dir)
    
    print("\n" + "="*70)
    print("ğŸš€ ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*70)
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"ë¸Œë¼ìš°ì € ëª¨ë“œ: {'ìˆ¨ê¹€' if args.headless else 'í‘œì‹œ'}")
    if categories:
        print(f"í¬ë¡¤ë§ ì¹´í…Œê³ ë¦¬: {', '.join(categories)}")
    else:
        print(f"í¬ë¡¤ë§ ì¹´í…Œê³ ë¦¬: ì „ì²´")
    print("="*70)
    
    # ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
    await crawler.init_browser(headless=args.headless)
    
    try:
        # ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§
        if args.store:
            await crawl_single_store(crawler, args.store, categories)
        
        # ì—¬ëŸ¬ ë§¤ì¥ í¬ë¡¤ë§
        elif args.stores:
            store_names = [s.strip() for s in args.stores.split(',')]
            await crawl_multiple_stores(
                crawler, 
                store_names, 
                categories,
                delay=args.delay
            )
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        await crawler.close_browser()
        print("\n" + "="*70)
        print("âœ… í¬ë¡¤ë§ ì‘ì—… ì¢…ë£Œ")
        print("="*70 + "\n")


if __name__ == "__main__":
    # Windowsì—ì„œ ì´ë²¤íŠ¸ ë£¨í”„ ì •ì±… ì„¤ì •
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main())


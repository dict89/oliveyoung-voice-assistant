"""
ì˜¬ë¦¬ë¸Œì˜ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬
ë§¤ì¥ ì •ë³´ ë° ìƒí’ˆ ì¬ê³  ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser
import re


class OliveYoungCrawler:
    """ì˜¬ë¦¬ë¸Œì˜ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬"""
    
    BASE_URL = "https://www.oliveyoung.co.kr"
    STORE_INFO_URL = f"{BASE_URL}/store/store/getStoreInfoMain.do"
    
    def __init__(self, output_dir: str = "data"):
        """
        Args:
            output_dir: í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
    async def init_browser(self, headless: bool = True):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=headless)
        context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        self.page = await context.new_page()
        
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.browser:
            await self.browser.close()
    
    async def get_store_list(self) -> List[Dict]:
        """
        ì˜¬ë¦¬ë¸Œì˜ ì „ì²´ ë§¤ì¥ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Returns:
            ë§¤ì¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ“ ë§¤ì¥ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        try:
            await self.page.goto(self.STORE_INFO_URL, wait_until="networkidle")
            await asyncio.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            stores = []
            
            # ë§¤ì¥ ëª©ë¡ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            # ì˜¬ë¦¬ë¸Œì˜ ì›¹ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ êµ¬ì¡°ë¥¼ í™•ì¸í•œ í›„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤
            store_elements = soup.select('.store-item, .storeInfo, li.store')
            
            for element in store_elements:
                try:
                    store_name = element.select_one('.store-name, .storeName, h3, .title')
                    store_addr = element.select_one('.store-address, .address, .addr')
                    store_id = element.get('data-store-id') or element.get('data-id')
                    
                    if store_name:
                        stores.append({
                            'store_id': store_id or '',
                            'name': store_name.text.strip(),
                            'address': store_addr.text.strip() if store_addr else '',
                        })
                except Exception as e:
                    print(f"ë§¤ì¥ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… {len(stores)}ê°œ ë§¤ì¥ ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return stores
            
        except Exception as e:
            print(f"âŒ ë§¤ì¥ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    async def search_store_by_name(self, store_name: str) -> Optional[str]:
        """
        ë§¤ì¥ëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë§¤ì¥ ì½”ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        
        Args:
            store_name: ê²€ìƒ‰í•  ë§¤ì¥ëª… (ì˜ˆ: "ëª…ë™ íƒ€ìš´")
            
        Returns:
            ë§¤ì¥ ì½”ë“œ ë˜ëŠ” None
        """
        print(f"ğŸ” '{store_name}' ë§¤ì¥ì„ ê²€ìƒ‰ ì¤‘...")
        
        try:
            await self.page.goto(self.STORE_INFO_URL, wait_until="networkidle")
            
            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ì…ë ¥
            search_input = await self.page.query_selector('input[name="storeName"], input#searchKeyword, input.search-input')
            if search_input:
                await search_input.fill(store_name)
                await asyncio.sleep(1)
                
                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ë˜ëŠ” ì—”í„°
                search_btn = await self.page.query_selector('button.search-btn, button[type="submit"]')
                if search_btn:
                    await search_btn.click()
                else:
                    await search_input.press('Enter')
                
                await asyncio.sleep(2)
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì²« ë²ˆì§¸ ë§¤ì¥ ì„ íƒ
            store_link = await self.page.query_selector('.store-item:first-child, .storeInfo:first-child')
            if store_link:
                store_code = await store_link.get_attribute('data-store-id')
                print(f"âœ… ë§¤ì¥ ì½”ë“œ: {store_code}")
                return store_code
            
        except Exception as e:
            print(f"âŒ ë§¤ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def get_store_products(self, store_name: str = "ëª…ë™ íƒ€ìš´", 
                                 categories: List[str] = None) -> Dict:
        """
        íŠ¹ì • ë§¤ì¥ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            store_name: ë§¤ì¥ëª…
            categories: í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        print(f"ğŸ›ï¸  '{store_name}' ë§¤ì¥ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘...")
        
        products = {
            'store_name': store_name,
            'crawled_at': datetime.now().isoformat(),
            'categories': {}
        }
        
        try:
            # ë§¤ì¥ í˜ì´ì§€ë¡œ ì´ë™
            await self.page.goto(self.STORE_INFO_URL, wait_until="networkidle")
            
            # ë§¤ì¥ ê²€ìƒ‰ ë° ì„ íƒ
            store_code = await self.search_store_by_name(store_name)
            if not store_code:
                print("âš ï¸  ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ íƒìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # "ì¬ê³ ì¡°íšŒ" ë˜ëŠ” "ìƒí’ˆë³´ê¸°" ë²„íŠ¼ í´ë¦­
            await asyncio.sleep(2)
            
            # ë°©ë²• 1: ì¬ê³  ì¡°íšŒ ë²„íŠ¼ ì°¾ê¸°
            inventory_btn = await self.page.query_selector(
                'button:has-text("ì¬ê³ "), a:has-text("ì¬ê³ "), .inventory-btn'
            )
            if inventory_btn:
                await inventory_btn.click()
                await asyncio.sleep(3)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ìˆ˜ì§‘
            default_categories = [
                'ìŠ¤í‚¨ì¼€ì–´', 'ë©”ì´í¬ì—…', 'ë§ˆìŠ¤í¬/íŒ©', 'í´ë Œì§•', 
                'ì„ ì¼€ì–´', 'í—¤ì–´ì¼€ì–´', 'ë°”ë””ì¼€ì–´', 'ë‚¨ì„±ì¼€ì–´'
            ]
            
            target_categories = categories or default_categories
            
            for category in target_categories:
                print(f"  ğŸ“¦ '{category}' ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘...")
                category_products = await self._get_category_products(category)
                products['categories'][category] = category_products
                await asyncio.sleep(1)
            
            # ê²°ê³¼ ì €ì¥
            output_file = self.output_dir / f"products_{store_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ìƒí’ˆ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
            
        except Exception as e:
            print(f"âŒ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return products
    
    async def _get_category_products(self, category_name: str) -> List[Dict]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            category_name: ì¹´í…Œê³ ë¦¬ëª…
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        products = []
        
        try:
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ (ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            category_btn = await self.page.query_selector(f'button:has-text("{category_name}"), a:has-text("{category_name}")')
            if category_btn:
                await category_btn.click()
                await asyncio.sleep(2)
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë™ì  ë¡œë”© ëŒ€ì‘)
            await self._scroll_page()
            
            # ìƒí’ˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ìƒí’ˆ ìš”ì†Œ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            product_elements = soup.select('.product-item, .prd_item, .goods-item, li.item')
            
            for element in product_elements:
                try:
                    product = await self._parse_product_element(element)
                    if product:
                        products.append(product)
                except Exception as e:
                    print(f"    ìƒí’ˆ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"    âœ… {len(products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"    âŒ ì¹´í…Œê³ ë¦¬ '{category_name}' ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return products
    
    async def _parse_product_element(self, element) -> Optional[Dict]:
        """
        ìƒí’ˆ ìš”ì†Œë¥¼ íŒŒì‹±í•˜ì—¬ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            element: BeautifulSoup ìš”ì†Œ
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ìƒí’ˆëª…
            name_elem = element.select_one('.prd-name, .prod_name, .product-name, .name, h3, .title')
            name = name_elem.text.strip() if name_elem else None
            
            # ê°€ê²©
            price_elem = element.select_one('.price, .prd-price, .sale-price, .cost')
            price_text = price_elem.text.strip() if price_elem else "0"
            price = int(re.sub(r'[^\d]', '', price_text)) if price_text else 0
            
            # ì´ë¯¸ì§€
            img_elem = element.select_one('img')
            image_url = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
            if image_url and not image_url.startswith('http'):
                image_url = self.BASE_URL + image_url
            
            # ë¸Œëœë“œ
            brand_elem = element.select_one('.brand, .prd-brand, .brand-name')
            brand = brand_elem.text.strip() if brand_elem else None
            
            # ì¬ê³  ìƒíƒœ
            stock_elem = element.select_one('.stock, .stock-status, .inventory')
            stock_status = stock_elem.text.strip() if stock_elem else "ì¬ê³  í™•ì¸ í•„ìš”"
            
            # ìƒí’ˆ ì½”ë“œ
            product_id = element.get('data-product-id') or element.get('data-goods-no')
            
            if name:
                return {
                    'product_id': product_id or '',
                    'name': name,
                    'brand': brand or '',
                    'price': price,
                    'image_url': image_url or '',
                    'stock_status': stock_status,
                }
        
        except Exception as e:
            print(f"      íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return None
    
    async def _scroll_page(self, scroll_count: int = 3):
        """í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸ ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        for i in range(scroll_count):
            await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(1)
    
    async def get_product_detail(self, product_url: str) -> Dict:
        """
        ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            product_url: ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URL
            
        Returns:
            ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            await self.page.goto(product_url, wait_until="networkidle")
            await asyncio.sleep(2)
            
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detail = {
                'description': '',
                'ingredients': [],
                'how_to_use': '',
                'reviews_count': 0,
                'rating': 0.0,
            }
            
            # ìƒí’ˆ ì„¤ëª…
            desc_elem = soup.select_one('.product-detail, .prd-detail, .description')
            if desc_elem:
                detail['description'] = desc_elem.text.strip()
            
            # ë¦¬ë·° ìˆ˜
            review_elem = soup.select_one('.review-count, .reviews-count')
            if review_elem:
                review_text = re.sub(r'[^\d]', '', review_elem.text)
                detail['reviews_count'] = int(review_text) if review_text else 0
            
            # í‰ì 
            rating_elem = soup.select_one('.rating, .score, .star-score')
            if rating_elem:
                rating_text = re.findall(r'[\d.]+', rating_elem.text)
                detail['rating'] = float(rating_text[0]) if rating_text else 0.0
            
            return detail
            
        except Exception as e:
            print(f"ìƒí’ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return {}
    
    async def crawl_all(self, store_names: List[str] = None):
        """
        ì—¬ëŸ¬ ë§¤ì¥ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ì¼ê´„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            store_names: ë§¤ì¥ëª… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì£¼ìš” ë§¤ì¥ë§Œ)
        """
        default_stores = ["ëª…ë™ íƒ€ìš´", "ëª…ë™ ì¤‘ì•™ì ", "ê°•ë‚¨ì—­ì ", "í™ëŒ€ì…êµ¬ì "]
        target_stores = store_names or default_stores
        
        await self.init_browser(headless=False)  # ë””ë²„ê¹…ì„ ìœ„í•´ ë¸Œë¼ìš°ì € í‘œì‹œ
        
        try:
            for store_name in target_stores:
                print(f"\n{'='*60}")
                print(f"ğŸª {store_name} ë§¤ì¥ í¬ë¡¤ë§ ì‹œì‘")
                print(f"{'='*60}\n")
                
                await self.get_store_products(store_name)
                await asyncio.sleep(3)  # ìš”ì²­ ê°„ê²©
                
        finally:
            await self.close_browser()
        
        print("\nâœ… ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = OliveYoungCrawler()
    
    # ëª…ë™ íƒ€ìš´ ë§¤ì¥ í¬ë¡¤ë§
    await crawler.init_browser(headless=False)
    try:
        await crawler.get_store_products(
            store_name="ëª…ë™ íƒ€ìš´",
            categories=['ìŠ¤í‚¨ì¼€ì–´', 'ë©”ì´í¬ì—…', 'ë§ˆìŠ¤í¬/íŒ©']
        )
    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    asyncio.run(main())

